{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07d56cad-a79d-449b-8441-ec50bb10f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "\n",
    "# !!!NEED TO DELETE!!!! JUST FOR DEV!!!!\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import time, random, numpy as np, argparse, sys, re, os\n",
    "from types import SimpleNamespace, new_class\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bert import BertModel\n",
    "from optimizer import AdamW\n",
    "from tqdm import trange\n",
    "\n",
    "import logging\n",
    "\n",
    "from PCGrad import PCGrad\n",
    "from datasets import SentenceClassificationDataset, SentencePairDataset, \\\n",
    "    load_multitask_data, load_multitask_test_data\n",
    "\n",
    "\n",
    "\n",
    "from evaluation import model_eval_sst, test_model_multitask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe3baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TQDM_DISABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb46eb7e-ee9e-49fe-b6b7-95c19e1096da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the random seed\n",
    "def seed_everything(seed=11711):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0d4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_HIDDEN_SIZE = 768\n",
    "N_SENTIMENT_CLASSES = 5\n",
    "\n",
    "\n",
    "class MultitaskBERT(nn.Module):\n",
    "    '''\n",
    "    This module should use BERT for 3 tasks:\n",
    "\n",
    "    - Sentiment classification (predict_sentiment)\n",
    "    - Paraphrase detection (predict_paraphrase)\n",
    "    - Semantic Textual Similarity (predict_similarity)\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(MultitaskBERT, self).__init__()\n",
    "        # You will want to add layers here to perform the downstream tasks.\n",
    "        # Pretrain mode does not require updating bert paramters.\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.bert.parameters():\n",
    "            if config.option == 'pretrain':\n",
    "                param.requires_grad = False\n",
    "            elif config.option == 'finetune':\n",
    "                param.requires_grad = True\n",
    "        ### TODO\n",
    "        self.sentiment_classifier = nn.Linear(self.bert.config.hidden_size, 5)\n",
    "        self.paraphrase_classifier = nn.Linear(self.bert.config.hidden_size * 2, 2)\n",
    "        self.similarity_classifier = nn.Linear(self.bert.config.hidden_size * 2, 1)\n",
    "        # raise NotImplementedError\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        'Takes a batch of sentences and produces embeddings for them.'\n",
    "        # The final BERT embedding is the hidden state of [CLS] token (the first token)\n",
    "        # Here, you can start by just returning the embeddings straight from BERT.\n",
    "        # When thinking of improvements, you can later try modifying this\n",
    "        # (e.g., by adding other layers).\n",
    "        ### TODO\n",
    "        bert_outputs = self.bert(input_ids, attention_mask)\n",
    "        return bert_outputs['pooler_output']\n",
    "        # ['last_hidden_state'][:, 0, :] ['pooler_output']\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def predict_sentiment(self, input_ids, attention_mask):\n",
    "        '''Given a batch of sentences, outputs logits for classifying sentiment.\n",
    "        There are 5 sentiment classes:\n",
    "        (0 - negative, 1- somewhat negative, 2- neutral, 3- somewhat positive, 4- positive)\n",
    "        Thus, your output should contain 5 logits for each sentence.\n",
    "        '''\n",
    "        ### TODO\n",
    "        embeddings = self.forward(input_ids, attention_mask)\n",
    "        return self.sentiment_classifier(embeddings)\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def predict_paraphrase(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit for predicting whether they are paraphrases.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "        ### TODO\n",
    "        embeddings_1 = self.forward(input_ids_1, attention_mask_1)\n",
    "        embeddings_2 = self.forward(input_ids_2, attention_mask_2)\n",
    "        concat_embeddings = torch.cat((embeddings_1, embeddings_2), dim=1)\n",
    "        return self.paraphrase_classifier(concat_embeddings)\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def predict_similarity(self,\n",
    "                           input_ids_1, attention_mask_1,\n",
    "                           input_ids_2, attention_mask_2):\n",
    "        '''Given a batch of pairs of sentences, outputs a single logit corresponding to how similar they are.\n",
    "        Note that your output should be unnormalized (a logit); it will be passed to the sigmoid function\n",
    "        during evaluation, and handled as a logit by the appropriate loss function.\n",
    "        '''\n",
    "        ### TODO\n",
    "        embeddings_1 = self.forward(input_ids_1, attention_mask_1)\n",
    "        embeddings_2 = self.forward(input_ids_2, attention_mask_2)\n",
    "        concat_embeddings = torch.cat((embeddings_1, embeddings_2), dim=1)\n",
    "        return self.similarity_classifier(concat_embeddings)\n",
    "        raise NotImplementedError\n",
    "    \n",
    "def save_model(model, optimizer, args, config, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'args': args,\n",
    "        'model_config': config,\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, filepath)\n",
    "    print(f\"save the model to {filepath}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0452c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Currently only trains on sst dataset\n",
    "# Curttently trains 3 task (use simple Objective func = LossFunc1+LF2+LF3) without Gradient Surgery\n",
    "def train_multitask(args):\n",
    "    device = torch.device('cuda') if args[\"use_gpu\"] else torch.device('cpu')\n",
    "    # Load data\n",
    "    # Create the data and its corresponding datasets and dataloader\n",
    "    sst_train_data, num_labels,para_train_data, sts_train_data = load_multitask_data(args[\"sst_train\"],args[\"para_train\"],args[\"sts_train\"], split ='train')\n",
    "    sst_dev_data, num_labels,para_dev_data, sts_dev_data = load_multitask_data(args[\"sst_dev\"],args[\"para_dev\"],args[\"sts_dev\"], split ='train')\n",
    "\n",
    "    sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "    sst_dev_data = SentenceClassificationDataset(sst_dev_data, args)\n",
    "\n",
    "    sst_train_dataloader = DataLoader(sst_train_data, shuffle=True, batch_size=args[\"batch_size\"],\n",
    "                                      collate_fn=sst_train_data.collate_fn)\n",
    "    sst_dev_dataloader = DataLoader(sst_dev_data, shuffle=False, batch_size=args[\"batch_size\"],\n",
    "                                    collate_fn=sst_dev_data.collate_fn)\n",
    "    \n",
    "    # I wrote this for multi-task learning\n",
    "    para_train_data = SentencePairDataset(para_train_data, args)\n",
    "    para_dev_data = SentencePairDataset(para_dev_data, args)\n",
    "    sts_train_data = SentencePairDataset(sts_train_data, args)\n",
    "    sts_dev_data = SentencePairDataset(sts_dev_data, args)\n",
    "    \n",
    "    para_train_dataloader = DataLoader(para_train_data, shuffle=True, batch_size=args[\"batch_size\"], collate_fn=para_train_data.collate_fn)\n",
    "    para_dev_dataloader = DataLoader(para_dev_data, shuffle=False, batch_size=args[\"batch_size\"], collate_fn=para_dev_data.collate_fn)\n",
    "\n",
    "    sts_train_dataloader = DataLoader(sts_train_data, shuffle=True, batch_size=args[\"batch_size\"], collate_fn=sts_train_data.collate_fn)\n",
    "    sts_dev_dataloader = DataLoader(sts_dev_data, shuffle=False, batch_size=args[\"batch_size\"], collate_fn=sts_dev_data.collate_fn)\n",
    "\n",
    "    \n",
    "\n",
    "    # Init model\n",
    "    config = {'hidden_dropout_prob': args[\"hidden_dropout_prob\"],\n",
    "              'num_labels': num_labels,\n",
    "              'hidden_size': 768,\n",
    "              'data_dir': '.',\n",
    "              'option': args[\"option\"]}\n",
    "\n",
    "    config = SimpleNamespace(**config)\n",
    "\n",
    "    model = MultitaskBERT(config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    lr = args[\"lr\"]\n",
    "    # !!!NEED TO BE MODIFY!!! from lr=0.0004 to lr=lr\n",
    "    # hyperparams: lr, betas, eps, weight_decay\n",
    "    # 0.38/0.315/0.009 =>(default) AdamW(model.parameters(), lr=1e-05, weight_decay=0, betas=[0.9, 0.999], eps=1e-06, correct_bias=True)\n",
    "    # 0.380/0.315/0.009=> optimizer = AdamW(model.parameters(), lr=1e-05, weight_decay=0.95, betas=[0.9, 0.999], eps=1e-8, correct_bias=True)\n",
    "    # 0.380/0.316/0.009 (pre-train)=> optimizer = AdamW(model.parameters(), lr=2e-05, weight_decay=0.95, betas=[0.9, 0.999], eps=1e-8)\n",
    "    # 0.380/0.322/0.009 =>(half the weighted decay) optimizer = AdamW(model.parameters(), lr=2e-05, weight_decay=0.5, betas=[0.9, 0.999], eps=1e-08)\n",
    "    # .../0.358/... => lr=1e-04,wd=0.9\n",
    "    # 0.367 wd0.05\n",
    "    # 0.322 2e-05/0.05\n",
    "    # 0.316 2e-05/0.95\n",
    "    # 0.4/50.38 => 4e-04/0.05 || multitask learning => 0.388 0.000 0.267\n",
    "    # Para 0/sst 0.317/sts 0.132 <= multi-task 2e-05 0.05 1e-08\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-04, weight_decay=0.05, betas=[0.9, 0.999], eps=1e-06)\n",
    "    pc_adam = PCGrad(optimizer)\n",
    "    best_dev_acc = 0\n",
    "\n",
    "    # Run for the specified number of epochs\n",
    "    master_bar = trange(args['epochs'])\n",
    "    for epoch in master_bar:\n",
    "        # print(f\"Training Epoch number {epoch}\")\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "        dataloader_iterator = zip(sst_train_dataloader, para_train_dataloader, sts_train_dataloader)\n",
    "        for (sst_batch, para_batch, sts_batch) in dataloader_iterator:\n",
    "            pc_adam.zero_grad()\n",
    "\n",
    "            # SST task\n",
    "            sst_ids, sst_mask, sst_labels = (sst_batch['token_ids'],\n",
    "                                        sst_batch['attention_mask'], sst_batch['labels'])\n",
    "            sst_ids, sst_mask,sst_labels = sst_ids.to(device),sst_mask.to(device), sst_labels.to(device)\n",
    "            sst_outputs = model.predict_sentiment(sst_ids, sst_mask)\n",
    "            # print(sst_outputs)\n",
    "            sst_loss = F.cross_entropy(sst_outputs, sst_labels.view(-1), reduction='sum') / args[\"batch_size\"]\n",
    "\n",
    "            # Paraphrase Task\n",
    "            para_id1, para_mask1, para_id2, para_mask2, para_labels = (para_batch['token_ids_1'],\n",
    "                                                            para_batch['attention_mask_1'], para_batch['token_ids_2'],para_batch['attention_mask_2'], para_batch['labels'])\n",
    "            para_id1, para_mask1, para_id2, para_mask2, para_labels = para_id1.to(device), para_mask1.to(device), para_id2.to(device), para_mask2.to(device), para_labels.to(device)\n",
    "            para_outputs = model.predict_paraphrase(para_id1, para_mask1, para_id2, para_mask2)\n",
    "            n_classes = 2\n",
    "            assert para_outputs.size(-1) == n_classes, f\"Output size should be {n_classes}, but got {para_outputs.size(-1)}\"\n",
    "            assert para_labels.max() < n_classes, f\"Max label is {para_labels.max()}, but should be less than {n_classes}\"\n",
    "            assert para_labels.min() >= 0, f\"Min label is {para_labels.min()}, but should be greater than or equal to 0\"\n",
    "            para_loss = F.cross_entropy(para_outputs, para_labels.view(-1), reduction='mean') / args[\"batch_size\"]\n",
    "\n",
    "            # STS Task\n",
    "            sts_id1, sts_mask1, sts_id2, sts_mask2, sts_labels = (sts_batch['token_ids_1'],\n",
    "                                                    sts_batch['attention_mask_1'], sts_batch['token_ids_2'],sts_batch['attention_mask_2'], sts_batch['labels'])\n",
    "            sts_id1, sts_mask1, sts_id2, sts_mask2, sts_labels = sts_id1.to(device), sts_mask1.to(device), sts_id2.to(device), sts_mask2.to(device), sts_labels.to(device)\n",
    "            sts_outputs = model.predict_similarity(sts_id1, sts_mask1, sts_id2, sts_mask2)\n",
    "            sts_outputs = torch.sigmoid(sts_outputs) * 5  # Scale to 0-5\n",
    "            # print(sts_outputs)\n",
    "            logging.info(\"Computing loss\")\n",
    "            sts_loss = F.mse_loss(sts_outputs.squeeze(), sts_labels.view(-1).float(), reduction='mean') / args[\"batch_size\"]\n",
    "\n",
    "            # Total loss and backprop \n",
    "            # total_loss = sst_loss + para_loss\n",
    "            \n",
    "            # Here we will compute the loss values for all the task in the end normalize all the loss vectors in a single direction. \n",
    "            total_loss = sst_loss + para_loss + sts_loss\n",
    "            # print(\"Merging the gradients\")\n",
    "            pc_adam.pc_backward([sst_loss, para_loss, sts_loss])\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += total_loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "\n",
    "        train_loss = train_loss / (num_batches)\n",
    "\n",
    "        train_acc, train_f1, *_ = model_eval_sst(sst_train_dataloader, model, device)\n",
    "        dev_acc, dev_f1, *_ = model_eval_sst(sst_dev_dataloader, model, device)\n",
    "\n",
    "        if dev_acc > best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            save_model(model, optimizer, args, config, args[\"filepath\"])\n",
    "\n",
    "        print(f\"Epoch {epoch}: train loss :: {train_loss :.3f}, train acc :: {train_acc :.3f}, dev acc :: {dev_acc :.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2392ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(args):\n",
    "    with torch.no_grad():\n",
    "        device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "        saved = torch.load(args.filepath)\n",
    "        config = saved['model_config']\n",
    "\n",
    "        model = MultitaskBERT(config)\n",
    "        model.load_state_dict(saved['model'])\n",
    "        model = model.to(device)\n",
    "        print(f\"Loaded model to test from {args.filepath}\")\n",
    "\n",
    "        test_model_multitask(args, model, device)\n",
    "\n",
    "def get_args():\n",
    "    args = {\n",
    "    \"sst_train\" : \"data/ids-sst-train.csv\",\n",
    "    \"sst_dev\" : \"data/ids-sst-dev.csv\",\n",
    "    \"sst_test\" : \"data/ids-sst-test-student.csv\",\n",
    "    \"para_train\" : \"data/quora-train.csv\",\n",
    "    \"para_dev\" : \"data/quora-dev.csv\",\n",
    "    \"para_test\" : \"data/quora-test-student.csv\",\n",
    "    \"sts_train\" : \"data/sts-train.csv\",\n",
    "    \"sts_dev\" : \"data/sts-dev.csv\",\n",
    "    \"sts_test\" : \"data/sts-test-student.csv\",\n",
    "    \"seed\" : 11711,\n",
    "    \"epochs\" : 10,\n",
    "    \"use_gpu\" : False,\n",
    "    \"option\" : \"pretrain\",\n",
    "    \"sst_dev_out\" : \"predictions/sst-dev-output.csv\",\n",
    "    \"sst_test_out\" : \"predictions/sst-test-output.csv\",\n",
    "    \"para_dev_out\" : \"predictions/para-dev-output.csv\",\n",
    "    \"para_test_out\" : \"predictions/para-test-output.csv\",\n",
    "    \"sts_dev_out\" : \"predictions/sts-dev-output.csv\",\n",
    "    \"sts_test_out\" : \"predictions/sts-test-output.csv\",\n",
    "    \"batch_size\": 8,\n",
    "    \"hidden_dropout_prob\" : 0.3,\n",
    "    \"lr\" : 2e-5\n",
    "}\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93ab3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain\n",
      "Loaded 8544 train examples from data/ids-sst-train.csv\n",
      "Loaded 141498 train examples from data/quora-train.csv\n",
      "Loaded 6040 train examples from data/sts-train.csv\n",
      "Loaded 1101 train examples from data/ids-sst-dev.csv\n",
      "Loaded 20212 train examples from data/quora-dev.csv\n",
      "Loaded 863 train examples from data/sts-dev.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:PC Grad Initialized\n",
      "  0%|          | 0/10 [11:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m args[\u001b[39m\"\u001b[39m\u001b[39mfilepath\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m\"\u001b[39m\u001b[39moption\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m-multitask.pt\u001b[39m\u001b[39m'\u001b[39m \u001b[39m# save path\u001b[39;00m\n\u001b[1;32m      5\u001b[0m seed_everything(args[\u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m])  \u001b[39m# fix the seed for reproducibility\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_multitask(args)\n\u001b[1;32m      7\u001b[0m test_model(args)\n",
      "Cell \u001b[0;32mIn[5], line 107\u001b[0m, in \u001b[0;36mtrain_multitask\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    105\u001b[0m total_loss \u001b[39m=\u001b[39m sst_loss \u001b[39m+\u001b[39m para_loss \u001b[39m+\u001b[39m sts_loss\n\u001b[1;32m    106\u001b[0m \u001b[39m# print(\"Merging the gradients\")\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m pc_adam\u001b[39m.\u001b[39;49mpc_backward([sst_loss, para_loss, sts_loss])\n\u001b[1;32m    108\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    110\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m total_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/github/DL-nlp/project/PCGrad.py:45\u001b[0m, in \u001b[0;36mPCGrad.pc_backward\u001b[0;34m(self, objectives)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39mcalculate the gradient of the parameters\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[39minput:\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m- objectives: a list of objectives\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     44\u001b[0m grads, shapes, has_grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pack_grad(objectives)\n\u001b[0;32m---> 45\u001b[0m pc_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_project_conflicting(grads, has_grads)\n\u001b[1;32m     46\u001b[0m pc_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unflatten_grad(pc_grad, shapes[\u001b[39m0\u001b[39m])\n\u001b[1;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_grad(pc_grad)\n",
      "File \u001b[0;32m~/github/DL-nlp/project/PCGrad.py:52\u001b[0m, in \u001b[0;36mPCGrad._project_conflicting\u001b[0;34m(self, grads, has_grads, shapes)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_project_conflicting\u001b[39m(\u001b[39mself\u001b[39m, grads, has_grads, shapes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mResolving conflicting losses\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     shared \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(has_grads)\u001b[39m.\u001b[39;49mprod(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mbool()\n\u001b[1;32m     53\u001b[0m     pc_grad, num_task \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(grads), \u001b[39mlen\u001b[39m(grads)\n\u001b[1;32m     54\u001b[0m     \u001b[39mfor\u001b[39;00m g_i \u001b[39min\u001b[39;00m pc_grad:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# args = get_args()\n",
    "args = get_args()\n",
    "print(args['option'])\n",
    "args[\"filepath\"] = f'{args[\"option\"]}-{args[\"epochs\"]}-{args[\"lr\"]}-multitask.pt' # save path\n",
    "seed_everything(args[\"seed\"])  # fix the seed for reproducibility\n",
    "train_multitask(args)\n",
    "test_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7108e57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated coordinates:\n",
      "x: 2.410661827045464\n",
      "y: 3.8124842360420828\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the reference points and distances\n",
    "# Format: (x, y, distance)\n",
    "ref_points = [(2, 2, 1.5), (5, 3, 2.8), (4, 7, 3.2)]\n",
    "\n",
    "# Gauss-Newton algorithm for trilateration\n",
    "def trilateration_gauss_newton(ref_points, max_iterations=100, threshold=1e-6):\n",
    "    # Initial estimate of (x, y)\n",
    "    x = 0.0\n",
    "    y = 0.0\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        residuals = []\n",
    "        jacobian = []\n",
    "        \n",
    "        # Compute residuals and Jacobian matrix\n",
    "        for ref_point in ref_points:\n",
    "            x_i, y_i, d_i = ref_point\n",
    "            r_i = np.sqrt((x - x_i)**2 + (y - y_i)**2)\n",
    "            \n",
    "            # Append residual\n",
    "            residuals.append(d_i - r_i)\n",
    "            \n",
    "            # Compute partial derivatives (Jacobian)\n",
    "            jacobian.append([(x - x_i) / r_i, (y - y_i) / r_i])\n",
    "        \n",
    "        # Convert residuals and Jacobian to numpy arrays\n",
    "        residuals = np.array(residuals)\n",
    "        jacobian = np.array(jacobian)\n",
    "        \n",
    "        # Solve linear system J^TJ Î”x = J^Tr\n",
    "        delta_x = np.linalg.inv(jacobian.T @ jacobian) @ jacobian.T @ residuals\n",
    "        \n",
    "        # Update estimate\n",
    "        x += delta_x[0]\n",
    "        y += delta_x[1]\n",
    "        \n",
    "        # Check convergence criteria\n",
    "        if np.linalg.norm(delta_x) < threshold:\n",
    "            break\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Run the trilateration algorithm\n",
    "estimated_x, estimated_y = trilateration_gauss_newton(ref_points)\n",
    "\n",
    "# Print the estimated coordinates\n",
    "print(\"Estimated coordinates:\")\n",
    "print(f\"x: {estimated_x}\")\n",
    "print(f\"y: {estimated_y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909f750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
